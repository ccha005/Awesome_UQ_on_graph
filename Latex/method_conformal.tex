\subsection{Conformal Prediction}
\subsubsection{Conformal Prediction Overview}\hfill\\
Conformal Prediction (CP), or Conformal Inference,  is an uncertainty quantification framework that can be applied to almost any model.  With a trained model and a pre-designed significant level $\alpha$, CP takes advantage of the experience of the calibration data to provide the estimated confidence intervals of predicted outcomes based on the scoring equations, which quantify the similarity, or conformity in other words, between the test data and calibration data. The theoretical essentials of CP are illustrated in "Algorithmic Learning in a Random World"\cite{vovk2005algorithmic} by Vladimir Vovk and coauthors in 2005. In this section, we will offer readers a generalized introduction to CP. First, we would like to help readers develop a basic understanding of the frameworks of CP on classification and regression models. Then, we talk about how to generate adaptive CP, which is a trending topic in this domain. We will also cover some content about the combination of CP and Bayesian inference and how to improve the stability of CP. Finally, the methods for evaluating CP will be discussed.

For CP on classification models, assume we have a trained classifier $\hat{f}$ which can output the estimated probability, such as softmax, for each of the total $K$ classes, $[0,1]^K$. Another calibration set of size $n$, $D_{cal}=(X_i,Y_i)_{i\in[n]}$, is applied to quantify how uncertainty the model $\hat{f}$ would be.  With the model $\hat{f}$ and the calibration dataset $D_{cal}$, we want to develop a prediction set $C(X_{test})$ of test input $X_{test}$ that satisfies
\begin{equation} 
P(Y_{test}\in C(X_{test})) \ge 1-\alpha\tag{4.2.1-1}
\end{equation}
Here we propose a critical assumption that $(X_i, Y_i)$ and $(X_{test}, Y_{test})$ are i.i.d. Some papers will also call $\alpha$ as the error rate or miscoverage rate, but here we stick with the terminology of statistics and call it significance level.

To accurately quantify the performance of $\hat{f}$ on $D_{cal}$, a conformal score is defined as 
\begin{equation} 
s_i = 1-\hat{f}(X_i)_{Y_i}\tag{4.2.1-2}
\end{equation}
As $\hat{f}(X_i)_{Y_i}$ is the softmax output of the true label $Y_i$, the conformal score $s_i$ will be low when $\hat{f}$ has high confidence in the correct prediction. In other words, $s_i$ will be high when the model is inaccurate. For each element of the calibration set $D_{cal}$, its corresponding conformal $s_i$ score will be calculated. The next key step is to compute the empirical quantile of the conformal score set $S = (s_i)_{i\in[n]}$.
 \begin{equation} 
Q_{1-\alpha}(S,D_{cal}):=(1-\alpha)(1+1/|D_{cal}|)\text{-th empirical quantile of }S \tag{4.2.1-3}
 \end{equation} 
Lastly, for a given $X_{test}$ whose $Y_{test}$ is unknown, the prediction set is $C(X_{test})=\{y|\hat{f}(X_{test})_y\ge 1-Q_{1-\alpha}\}$.  This prediction set will contain all predicted labels whose softmax outputs are higher than the empirical quantile value to meet the requirement of (4.2.1-1).

The procedure of applying CP on a regression model is similar to the process mentioned above. However, the prediction sets in this case become intervals instead of assembles of predicted labels. Again, let us begin with a trained regressor $\hat{\mu}$ and a calibration dataset $D_{cal}$ that the model has not seen before.  To construct a prediction set that satisfies (4.2.1-1), we also need to calculate the conformal score of $D_{cal}$, but this time we define it as the residual between the predicted values and the true value.
$r_i = |\hat{\mu}(X_i)-Y_i|$.  The empirical quantile on $R = (r_i)_{i\in[n]}$ is
 \begin{equation} 
Q_{1-\alpha}(R,D_{cal}):=(1-\alpha)(1+1/|D_{cal}|)\text{-th empirical quantile of }R \tag{4.2.1-5}
 \end{equation} 
Finally, the predicted interval of i.d.d. $X_{test}$ with given $\alpha$ can be written as $C(X_{test})=[\hat{\mu}(X_{test})-Q_{1-\alpha}(R,D_{cal}),\hat{\mu}(X_{test})+Q_{1-\alpha}(R,D_{cal})]$\\

One weakness of traditional CP is that it takes all elements within the calibration set to quantify the uncertainty of the trained model. This may cause the model to be overconfident in hard examples but underconfident in easy examples.  However, since we know the value of $X_{test}$, we can make use of the information to make the prediction set $C(X_{test})$ more adaptive to the test input. 

Designing novel conformal score equations is a widely applied method to improve the adaptiveness of prediction sets. In \cite{romano2020classification}, the authors revised the score equation (4.2.1-2) to generate adaptive prediction sets for classification models.  We follow the setup of the model $\hat{f}$ discussed above. For each element $X_i$ in $D_{cal}$, the trained model $\hat{f}$ can output the softmax result for all potential $K$ classes, $\hat{f}(X_i)_{y_k}=\pi_{y_k}(X_i), k=1,2,..,K$.  During this process, we assign the indices $\{1,2,.., K\}$ to elements within the set $(\pi_{y_k}(X_i))_{k\in[K]}$ from high to low, which means $\pi_{y_1}(X_i)$ is the possibility of the most likely class of $X_i$, whereas $\pi_{y_K}(X_i)$ is the least possible output.  Then, the conformal score is defined as the summation of the first $M$ elements where $y_M=Y_i$, which is the true class of $X_i$. 
\begin{equation}
s_i = \sum_{j=1}^M \pi_{y_j}(X_i)\text{, where } y_M=Y_i\\\tag{4.2.1-6}
\end{equation}
In other words, the score equation will keep including the softmax results from high to low until meets the true class. We still define $S$ as the set of $s_i$ and then calculate the quantile of $S$ via (4.2.1-3). With this quantile value, we will form the prediction set $C(X_{test})=\{y_1,..,y_M,y_{M+1}|M:\sup(\sum_{j=1}^M \pi_{y_j}(X_i))<Q_{1-\alpha}\}$. $y_{M+1}$ is included in  $C(X_{test})$ to avoid empty sets. \cite{angelopoulos2020uncertainty} is a well-cited publication using this adaptive prediction method for image classification tasks.

Applying localizers involved with the neighbor information of test data is also an effective way to make predicted intervals more adaptive. For instance, in \cite{guan2023localized}, the localizer for regression problems, with score $r$, is defined as the following. This localizer can give conformal scores closer to the test input higher weights.
\begin{equation}
H(X_{test},X_i)=e^{-5|X_i-X{test}|}\\\tag{4.2.1-7}
\end{equation}
Let $\delta _r$ denote the point mass at $r$. The weighted conformal score distribution according to the test input $X_{test}$ is
\begin{equation}
    \mathcal{F}_{test}:=(\sum_{i=1}^n p_{test,i}^H \delta_r_i) \text{ for i = 1...n}\text{ , where }p_{test,i}^H=\frac{H_{test,i}}{\sum_{j=1}^n H_{test,j}}\\\tag{4.2.1-8}
\end{equation}

Notice that all the algorithms above are built on the assumption that test data comes from the same distribution of the calibration set, which means $(X_i, Y_i)$ and $(X_{test}, Y_{test})$ are exchangeable. However, the probability distributions between calibration data $D_{cal}$ and test data $D_{test}$ can be different in the real world. The covariant shift is a typical situation in which the distribution of input values changes between $D_{cal}$ and $D_{test}$, e.g. $P_{D_{cal}}(X)\neq P_{D_{test}}(X)$, but their conditional probabilities of obtaining the same output are identical, e.g. $P_{D_{cal}}(Y|X) = P_{D_{test}}(Y|X)$.  \cite{tibshirani2019conformal} introduced using the likelihood ratio between $P_{D_{test}}(X)$ and $P_{D_{cal}}(X)$ to calculate the corresponding weight of each conformal score from the calibration set. To be specific, we define $w(x)=\frac{dP_{D_{test}}(X)}{dP_{D_{cal}}(X)}$, then the weights are in (4.2.1-8). These weights will assign higher priority to conformal scores which are more likely to appear in the test probability distribution. The quantile can be rewritten from (4.2.1-5) to illustrate the influence of $p_{cal}$, as (4.2.1-9).
\begin{equation}
p_{cal}^i(x)=\frac{w(X_i)}{\sum_{j=1}^n w(X_j)+w(x)}\text{, and } p_{test}(x)=\frac{w(x)}{\sum_{j=1}^n w(X_j)+w(x)}\\\tag{4.2.1-8}
\end{equation}
\begin{equation}
Q(x) = inf\{R_j,\sum_{i=1}^j p_{cal}^i(x)\mathbbm{1}\{R_i<R_j\}\leq 1-\alpha \}\\\tag{4.2.1-9}
\end{equation}

Another kind of probability distribution difference is called distribution shift like the calibration distribution and test distribution keep shifting along time series. \cite{zargarbashi2023conformal} proposed an adaptive conformal inference method that can adjust the miscoverage value $\alpha$ and prediction size according to the historical data.\cite{Barber2022ConformalPB} designed a nonsymmetric algorithm that treats recent observations as more relevant.

In the previous discussion, we begin with a well-trained model $\hat{f}$, but in fact, how to wisely split the known data $D_{known}$ into training data $D_{train}$ to train $\hat{f}$ and calibration data $D_{cal}$ to calculate conformal score is a long-standing concern for CP. Below we briefly introduce the works for data processing and CP implementation to improve CP's robustness. 

Full conformal prediction (FCP) is the first proposed method for conducting CP. With training data $D_{train}$ of $(X_i,Y_i)$ and a test input $X_{test}$, the trained model $f^z$ is based on $D_{train}\cup (X_{test},z) $, where $z$ is all possible $Y_i$ labels appear in $D_train$, so multiple models need to be trained. For each $f^z$, the conformal scores are $r_i=|Y_i-f^z(X_i)|$ and $r_{test}=|z-f^z(x_{test})|$. If $r_{test}$ is within the $1-\alpha$ quantile of conformal scores, we put the corresponding z to the prediction set of $X_test$. Full CP is computationally expensive and split conformal prediction (SCP) is developed to reduce the cost. As we discussed, SCP divides the known data $D_{known}$ into training data $D_{train}$ and calibration data $D_{cal}$ and calculates the prediction set based on the conformal scores from $D_{cal}$. However, such division may lead to randomness and unreliability of prediction output due to the reduced sample size for training and calibration phases. To solve this problem, \cite{Sesia2019ACO} suggested that using between 70$\%$ and 90$\%$ of the data for training often achieves a good balance between minimizing the size of the prediction intervals and the variability of practical coverage. Some cross-validation-related works are also designed to improve prediction robustness, such as cross-conformal prediction\cite{Vovk2012CrossconformalP}, and CP with jackkneif+ \cite{Barber2019PredictiveIW}. Stable CP \cite{ndiaye2022stable} states that the robustness of CP can be characterized by the trained model's stability $\tau$. 
\subsubsection{Conformal Prediction for Graphs}
The following definitions are for General ML.

% \noindent \textbf{Definition for General ML.}
Given a dataset $\mathcal{D}$, one could split it into four disjoint sets, a training set $\mathcal{D}_{train}$, a calibration set $\mathcal{D}_{cal}$, a validation set $\mathcal{D}_{val}$, and a test set $\mathcal{D}_{test}$, such that $\mathcal{D} = \mathcal{D}_{train} \bigcup \mathcal{D}_{cal} \bigcup \mathcal{D}_{val} \bigcup \mathcal{D}_{test}$.
Let $\hat{y}_i= f(X_i) \in \Delta^K$ be a probability distribution of $K$ classes predicted by the classifier, e.g., a GNN, on $\mathcal{D}_{train}$.
% 

\begin{definition}[Exchangeability]
\label{def:exchangeability}
    For any $z_1, \dots, z_n$ and any permutation $\zeta$ of $\{1, \dots, n\}$, 
    $\mathbb{P} \left( (Z_{\zeta(1)}, \dots, Z_{\zeta(n)}) = (z_1, \dots, z_n) \right) = 
    \mathbb{P} \left( (Z_1, \dots, Z_n) = (z_1, \dots, z_n) \right)$ holds. 
\end{definition}

Given that the calibration set with $n$ data $\mathcal{D}_{cal}=\{ (X_1, y_1), \dots, (X_n, y_n) \}$ and an unseen test data $X_{n+1}$, if $\mathcal{D}_{cal} \bigcup {(X_{n+1}, y_{n+1})}$ is exchangeable, then we can construct a prediction set with coverage guarantee $P(y_{n+1}\in \mathcal{C}(X_{n+1})) \geq 1 - \alpha$ for any specific significance level $\alpha$.
Formally, \cite{vovk2005algorithmic} shows that
\begin{theorem}
    Given an exchangeable set with $n+1$ data $\{ (X_i, y_i)\}_{i=1}^{n+1}$, 
    any score function $s: \mathcal{X}\times \mathcal{Y}\to \mathbb{R}$ and any specific significance level $\alpha \in (0, 1)$,
    define quantile $\hat{q}=\textnormal{Quantile}\left( \frac{ \lfloor (n-1)(\alpha) \rfloor }{n}; \{ s(x_i, y_i) \}_{i=1}^n \right)$ and prediction sets 
    $\mathcal{C}(X_{n+1}) = \{ y: s(X_{n+1},y) \geq \hat{q} \}$.
    We have
    \begin{equation}
        1-\alpha \leq P \left( y_{n+1} \in \mathcal{C}(X_{n+1}) \right) \leq 1- \alpha + \frac{1}{(n+1)}.
    \end{equation}
\end{theorem}
\textcolor{blue}{To add other operations: build quantile and the prediction set, etc.}

The score function $s(X, y)$ measures how $y$ ``conforms'' to the prediction at $X$.
Selecting a suitable score function is one of the challenges in conformal prediction, especially for conformal prediction in the graphs. 
% 
One popular choice is adaptive prediction sets (APS).
APS first sorts the predicted distribution into descending order, such that 
$\pi_{\zeta(1)}(X) \geq \pi_{\zeta(2)}(X) \geq \dots \geq \pi_{\zeta(K)}(X)$.
Then score function is defined as $s(X,y) = \sum_{j} \pi_{\zeta(j)}(X)$,
and the corresponding prediction set is constructed as
\begin{equation}
    \mathcal{C}(X)=\{ \zeta(1), \dots, \zeta(k^\ast) \}, \quad
    \textnormal{where} \; k^\ast=\inf\{ k: \sum_{j=1}^k \pi_{\zeta(j)}(X) \geq \hat{q} \}.
\end{equation}

% 
One may utilize a uniform random value to break potential ties between scores \cite{stutz2021learning}.


\noindent\textbf{Graph and Exchangeability.} 

For node classification tasks on graphs, 
the exchangeability may be violated for some specific settings \cite{zargarbashi2023conformal}.
First, 
in the \textit{transductive} settings, 
the model has access to the entire graph structure and all the node features during training, calibration, and testing, 
but the labels of calibration and testing set are not available during training. 
In this case, the union of calibration and the unlabeled sample is exchangeable.
However, 
in the \textit{inductive} settings, 
the model only has access to the subgraph induced by the nodes in the training set $\mathcal{D}_{train}$ during training.
The assumption of exchangeability does not hold in this case.


In the study \cite{zargarbashi2023conformal},
a diffused score function, 
namely Diffused Adaptive Prediction Sets (DAPS), 
is introduced. 
DAPS exploits the graph structure based on neighborhood diffusion,
and the diffused score function is defined as
\begin{equation}
    \hat{s}(X_i,y)=(1-\lambda)s(X_i,y) + \frac{\lambda}{|\mathcal{N}_i|} \sum_{X_j\in\mathcal{N}_i} s(X_j, y),
\end{equation}
where $\lambda$ signifies the diffusion parameter. 
From a matrix perspective, 
given the original score matrix $H\in\mathbb{R}^{|V|\times K}$, 
where each row is the conformal score for a node, 
the (1-hop) diffused score matrix is defined as 
\begin{equation}
    \hat{H} = (1-\lambda)H + \lambda D^{-1}AH,
\end{equation}
where $D$ is the degree matrix.
Extending it to a general $k$-hop diffused version,
the formulation is given by 
\begin{equation}
    \hat{H}=\lambda_0 H + \sum_{i=1}^k \lambda_i(D^{-1}A)^i \times H.
\end{equation}

Note that the diffused score function preserves exchangeability if the original scores are exchangeable. 
This diffusion process proves particularly advantageous in homophilous graphs, 
where connected nodes exhibit similar ground truth distributions. 
% 
The rationale behind this is that for a node surrounded by mostly unperturbed neighbors, 
its probability can be better estimated by using its neighbors' vectors (information), as long as these vectors are sufficiently similar.

For the non-exchangeable conformal prediction cases,
the procedure assumes a choice of deterministic fixed weights $w_1,\dots,w_n \in [0,1]$ \cite{barber2023conformal}.
The prediction set concerning the weighted 
quantiles of the score distribution 
\begin{equation}
\label{eq:cp_nonexchange}
    \hat{\mathcal{C}}(X) = \left\{ y\in\mathcal{Y}: s(X,y)\leq  \hat{q}_{1-\alpha} \left( \sum_{i=1}^n w_i \delta_{s_i}
    % + w_{n+1}\delta_{+\infty} 
    \right) \right\}
\end{equation}

The inductive cases are studied in \cite{clarkson2023distribution},
where the principle of exchangeability is not applicable.
They integrate non-exchangeable conformal prediction with the concept of homophily in graphs, 
leading to the development of three distinct variants of Neighbourhood Adaptive Prediction Sets (NAPS) for the prediction set construction.
The primary variant, NAPS, assigns the weights in Eq. (\ref{eq:cp_nonexchange}) to
$w_i=1$ if $X_i \in \mathcal{N}_{n+1}^T $, where $\mathcal{N}_{n+1}^T$ includes all nodes within $T$-hop neighborhood of node $X_{n+1}$. 
The second one, NAPS-H, adopts a hyperbolic decay rate for $w_i=t^{-1}$ where $t$ is the $t$-hop neighbor of node $X_{n+1}$.
The third one, NAPS-G, uses a geometric decay rate for $w_i=2^{-t}$.
