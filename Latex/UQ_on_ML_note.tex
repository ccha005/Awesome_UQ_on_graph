\documentclass{article}
\usepackage{graphicx} % Required for inserting images

\title{UQML}
\author{Rui Xu}
\date{November 2023}
\usepackage{amsmath}
\begin{document}

\maketitle

\section{Reviews}
\subsection{Calibrating Uncertainty Models for Steering Angle Estimation - 2019 IEEE}
This paper reviews three main kinds of UQ methods in machine learning: Monte Carlo Dropout, Bootstrap Model, and Gaussian Mixture Model. Evaluation metrics are also introduced. Prediction accuracy, usually chosen as RMSE or MAE, is used to see if the predicted value is close to the training data. Calibration, on the other hand, is to measure if the predicted distribution is similar to the training data distribution.  
\subsubsection{Monte Carlo Dropout}
\begin{figure}[ht]
\includegraphics[width=3in]{MC dropout.png}
\end{figure}
\noindent To construct the dropout variant based on the baseline model, dropout layers are added after every fully connected layer but the last. Dropout is implemented by sampling a Bernoulli random variable with probability p. Neuron k in layer l thus gets dropped with probability $z^{l;k}$  $\sim$ Bernoulli(p).\\

\noindent Inference for an unknown sample x is performed by doing multiple stochastic forward passes and averaging the result. The randomness comes from generating new dropout weight matrices $\hat{W}_t$ from the Bernoulli distribution for every forward pass t. A forward pass with network weights $\hat{W}_t$ is denoted as $f ^{\hat{W}_t}(x)$. The predicted mean steering angle can then be calculated as
\[ E[\hat{y}]=\frac{1}{T}\sum_{t=1}^{T}\hat{y}_t=\frac{1}{T}\sum_{t=1}^{T}f ^{\hat{W}_t}(x) \]
We can also get the predictive variance of the stochastic forward passes, seen as our uncertainty estimate, by calculating the variance over all T results plus the inverse model
precision $\tau^{-1}$ as
\[Var[\hat{y}]=\tau^{-1}+\frac{1}{T}\sum_{t=1}^{T}(f ^{\hat{W}_t}(x))^2-(E[\hat{y}])^2\]
The model precision $\tau^{-1}$ can be interpreted as label noise and thus representing the aleatoric uncertainty. Being a fixed constant it implies homoscedastic aleatoric uncertainty. We omit the model precision and use the variance of the outputs as predictive uncertainty.
\subsubsection{Bootstrap Model}
We would have to train models separately and make inferences in those networks K in parallel, we rely on a simplification instead: multiple models are fused into one model by sharing the visual encoder network.\\

\noindent The process of sampling K subsets of the training data for each subnetwork is done by generating a mask for every sample indicating which head it is passed to during training.\\

\noindent This ensures that every head sees only a subset of the whole training data. The data subsampling is realized by generating a binary mask $\{0, 1\}^K$ for every training sample indicating whether it is seen by head $k \in [1, K]$.\\

\begin{figure}[ht]
\includegraphics[width=3in]{Bootstrap.png}
\end{figure}

\noindent For sample t and head k, this results in variable $m^{k}_t\sim$ Bernoulli(p) with p = 0:5 specifying whether a head is trained on a certain sample.\\

\noindent The loss thus is defined as
\[L(\theta)=\sum_{i=1}^{N}\sum_{k=1}^{K}m^k_i\frac{1}{2}||y_i-\hat{y}_i||^2\]

\noindent Inference is performed by forward-propagation through all heads resulting in K outputs. Note that only a single pass is required to get K outputs, instead of having to do K passes as in the dropout model. $f^k(x)$ is the prediction of head $k$ for sample $x$. 
\[E[\hat{y}]=\frac{1}{K}\sum_{k=1}^{K}f^k(x)\]
\[Var[\hat{y}]=\frac{1}{K}\sum_{k=1}^{K}(f^k(x))^2-(E[\hat{y}])^{2}\]

\noindent Bootstrap model can not get aleatoric and epistemic uncertainty separated. Instead, it only yields the predictive variance which is used as predictive uncertainty. The inference step is computationally more expensive than inference in the baseline model as there are K more computations in the fully connected layer.
\subsubsection{Gaussian Mixture Model}
A Gaussian Mixture Model is composed of multiple Gaussians combined in a weighted sum. The parameters of a GMM are $\theta = \pi_i; \mu_i; \sigma^2_i; i \in [1, K]$ for $K$ mixtures.
\begin{center}
\includegraphics[width=2in]{GMM.png}
\end{center}
\noindent The mixture model is trained using the negative log-likelihood as a loss. A small constant 
$\epsilon$ = 1e - 6 is added to the argument of the logarithm for numerical stability.
\[L(\theta)=-\frac{1}{N}\sum_{i=1}^{N}\log(p(y_i|x_i))=\-\frac{1}{N}\sum_{i=1}^{N}\log(\sum_{j=1}^{K}\pi_j^{(i)}N(y_i|\mu_j^{(i)},\sigma_j^{2(i)})+\epsilon)\]
\noindent All mixture components are combined in a weighted sum and we get the following results for the expected value:
\[E[\hat{y}]=\sum_{j=1}^{K}\pi_j(x)\mu_j(x)\]
\noindent The total variance, in our context, called the predicted variance which is interpreted as predictive uncertainty, decomposes into the weighted sum of the variances and the weighted variances of the means,
\[Var[\hat{y}]=\sum_{j=1}^{K}\pi_j(x)\sigma_j^{2}(x)+\sum_{j=1}^{K}\pi_j(x)||\mu_j(x)-\sum_{k=1}^{K}\pi_k(x)\mu_k(x)||^{2}\]
\noindent The second term is also referred to as explained variance or epistemic uncertainty as it vanishes with more data. A decreasing aleatoric uncertainty can be interpreted as a decreasing variance of the mixture components. In turn, intuitively, a decreasing epistemic uncertainty equals to the means of the mixture components getting closer. However, this is only the case as long as there are no ambiguous situations as would be the case at intersections with multiple possible routes.


\subsubsection{Prediction Accuracy}
The prediction accuracy is usually chosen as root mean square error(RMSE) or mean absolute error(MAE) to measure the distance between the predicted values and true values. 
\subsubsection{Calibration}
\noindent Intuitively, this means that the uncertainty estimations have to be a true probability and should reflect the true likelihood.
\begin{center}
\includegraphics[width=2.75in]{Calibration.png}
\end{center}
\noindent First, a z\% confidence interval is computed for each sample in the data set based on the predicted mean and variance. The confidence interval (CI) specifies the lower and upper bound arranged symmetrically around the mean, containing z\% of the given Gaussian distribution mass.

\section{Researches}
\subsection{Conformalized Quantile Regression 2019 NIPS}
\subsubsection{Quantile Regression}
The conditional distribution function of $Y$ given $X$ = $x$ is
\[F(y|X=x)=P\{Y<y|X=x\}\]
and that the $\alpha$th conditional quantile function is
\[q_\alpha(x)=inf\{y\in R:F(y|X=x)\ge\alpha\}\]
Fix the lower and upper quantiles to be equal to $\alpha_{lo}$= $\alpha$/2 and $\alpha_{hi}$= 1-$\alpha$/2
Given the pair of lower and upper conditional quantile functions, we obtain a conditional prediction interval for $Y|$given $X=x$, with miscoverage rate $\alpha$.
\[C(x)=[q_{\alpha_{lo}}(x),q_{\alpha_{hi}}(x)]\]
By construction, this interval satisfies
\[P\{Y\in C(x)|X=x\}\ge 1-\alpha\]
Classical regression analysis estimates the conditional mean of the test response $Y_{n+1}$ given the features $X_{n+1}=x$ by minimizing the sum of squared residuals on the n training points:
\[\hat{\mu}=\mu(x;\hat{\theta})\]
\[\hat{\theta}=\operatorname*{argmin}_\theta \frac{1}{n}\sum_{i=1}^{n}(Y_i-\mu(X_i;\theta))^2+R(\theta)\]
Analogously, quantile regression estimates a conditional quantile function $q_\alpha$.
\[\hat{q_\alpha}(x)=f(x;\hat{\theta})\]
\[\hat{\theta}=\operatorname*{argmin}_\theta \frac{1}{n}\sum_{i=1}^{n}\rho_{\alpha}(Y_i,f(X_i;\theta))+R(\theta)\]
The loss function is called "pinball loss"
\[\rho_\alpha(y,\hat{y})=
\begin{cases}
\alpha(y-\hat{y}) & y-\hat{y}>0\\
(1-\alpha)(\hat{y}-y) & otherwise
\end{cases}
\]
All this suggests an obvious strategy to construct a prediction band with nominal miscoverage rate$\alpha$: estimate $\hat{q}_{\alpha_{lo}}(x)$ and $\hat{q}_{\alpha_{hi}} (x)$ using quantile regression, then output $C(X_{n+1})=[\hat{q}_{\alpha_{lo}} (X_{n+1}); \hat{q}_{\alpha_{hi}} 
(X_{n+1})]$ as an estimate of the ideal interval $C(X_{n+1})$.

\subsubsection{Conformal Prediction}
The split conformal method begins by splitting the training data into two disjoint subsets: a proper training set$\{(X_i, Y_i):i\in I_1\}$and calibration set$\{X_i, Y_i):i\in I_2\}$. Then, given any regression algorithm A, the regression model needs to fit the training set.
\[\hat{\mu}\leftarrow A(\{(X_i, Y_i):i\in I_1\})\]
the absolute residuals are computed on the calibration set, as follows:
\[R_i=|Y_i-\hat{\mu}(X_i)|, i\in I_2\]
For a given level $\alpha$, we then compute a quantile of the empirical distribution of the absolute residuals
\[Q_{i-\alpha}(R,I_2)=(1-\alpha)(1+1/|I_2|) of \{R_i:i \in I_2\}\]
The final prediction interval of new point $X_{n+1}$ is given by
\[C(X_{n+1})=[\hat{\mu}(X_{n+1})-Q_{1-\alpha}(R,I_2),\hat{\mu}(X_{n+1})+Q_{1-\alpha}(R,I_2)]\]
The length of $C(X_{n+1})$ is fixed and equal to $2Q_{1-\alpha}(R,I_2)$, independent of $X_{n+1}$.
\subsubsection{Conformalized quantile regression(CQR)}
As in split conformal prediction, we begin by splitting the data into a
proper training set, indexed by $I_1$, and a calibration set, indexed by $I_2$.I2. Given any quantile regression algorithm $A$, we then fit two conditional quantile functions $\hat{q}_{\alpha_{lo}}$ and $\hat{q}_{\alpha_{hi}}$ on the proper training set.In the essential next step, we compute conformity scores that quantify the error made by the plug-in prediction interval $\hat{C}(x)=[\hat{q}_{\alpha_{lo}}(x),\hat{q}_{\alpha_{hi}}(x)]$. The scores are evaluated on the calibration set as
\[E_i=max\{\hat{q}_{\alpha_{lo}}(X_i)-Y_i,Y_i-\hat{q}_{\alpha_{hi}}(X_i)\}\]
if $Y_i$ is below the lower endpoint of the interval, $Y_i<\hat{q}_{\alpha_{lo}}(X_i)$, then $E_i=\hat{q}_{\alpha_{lo}}(X_i)-Y_i$ is the magnitude of the error incurred by the mistake. Similarly, if $Y_i$ is above the upper endpoint of the interval, $Y_i>\hat{q}_{\alpha_{hi}}(X_i)$, then$E_i=Y_i-\hat{q}_{\alpha_{hi}}(X_i)$. Finally, if $Y_i$ is correctly within the interval, then $E_i$ is the larger value of the two non-positive numbers and so is itself non-positive.\\
Finally, given new input data $X_{n+1}$, we construct the prediction interval of $Y_{n+1}$ as 
\[C(X_{n+1})=[\hat{\mu}(X_{n+1})-Q_{1-\alpha}(E,I_2),\hat{\mu}(X_{n+1})+Q_{1-\alpha}(E,I_2)]\]

\subsection{Deep Bayes Active Learning with Image Data ICML 2017}
In active learning, a model is trained on a small amount of data (the initial training set), and an acquisition function (often based on the model's uncertainty) decides which data points to ask an external oracle for a label. In this paper, we concentrate on high-dimensional image data and need a model able to represent prediction uncertainty on such data.\\

\noindent These Bayesian CNNs are CNNs with prior probability distribution placed over a set of model parameters $w=\{W_1,...W_2\}$
\[w\sim p(w)\]
with for example a standard Gaussian prior $p(w)$. We further define a likelihood model
\[p(y=c|x,w)=softmax(f^w(x))\]
To perform approximate inference in the Bayesian CNN model we make use of stochastic regularisation techniques such as dropout. Inference is done by training a model with dropout before every weight layer, and by performing dropout at test time as well to sample from the approximate posterior. The uncertainty in the weights induces prediction uncertainty by marginalizing over the approximate posterior using Monte Carlo integration.
\begin{align*}
p(y=c|x,D_{train}) &=\int p(y=c|x,w)p(w|D_{train})dw\\
& \approx \int p(y=c|x,w)q_\theta(w)dw\\
& \approx \frac{1}{T}\sum_{t=1}^{T}p(y=c|x,\hat{w}_t)
\end{align*}

\noindent with $\hat{w}_t\sim q_\theta(w)$, where $q_\theta(w)$ is the dropout distribution.\\

\noindent Given a model $M$, pool data $D_{pool}$, and inputs $x \in D_{pool}$, an acquisition function $a(x;M)$ is a function of $x$ that the
AL system is used to decide where to query next:
\[x=\operatorname*{argmax}_{x\in D_{pool}} a(x,M)\]
Choose pool points that maximize the predictive entropy
\[H[y|x,D_{train}]=-\sum_c p(y=c|x,D_{train}) \log p(y=c|x,D_{train})\]
Use our approximation distribution $q_\theta(w)$, we can write the acquisition function as follows:
\begin{align*}
I[y,w|x,D_{train}] &=H[y|x,D_{train}]-E_{p(w|D_{train})}[H[y|x,w]]\\
& =-\sum_c p(y=c|x,D_{train})\log p(y=c|x,D_{train})\\
& + E_{p(w|D_{train})}[\sum_c p(y=c|x,D_{train})\log p(y=c|x,D_{train})]
\end{align*}
\noindent with $c$ the possible classes $y$ can take.\\
\noindent In the setting of $p(y=c|x,D_{train})=\int p(y=c|x,w)p(w,D_{train})dw$, we can rewrite:
\begin{align*}
I[y,w|x,D_{train}] &= -\sum_c \int p(y=c|x,w)p(w,D_{train})dw * \log \int p(y=c|x,w)p(w,D_{train})dw\\
& + E_{p(w|D_{train})}[\sum_c p(y=c|x,w)\log p(y=c|x,w)]
\end{align*}
\noindent Also, we use dropout distribution as an approximator:
\begin{align*}
I[y,w|x,D_{train}] &=\sum_c \int p(y=c|x,w)q_\theta(w)dw \log \int p(y=c|x,w)q_\theta(w)dw\\ 
& + E_{q_\theta(w))}[\sum_c p(y=c|x,w)\log p(y=c|x,w)]\\
& \approx \sum_c (\frac{1}{T}\sum_t\hat{p}_c^t)\log (\frac{1}{T}\sum_t\hat{p}_c^t)+\frac{1}{T}\sum_{c,t}\hat{p}_c^t\log\hat{p}_c^t)\
\end{align*}
\end{document}
