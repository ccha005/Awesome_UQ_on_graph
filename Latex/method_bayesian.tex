% \section{Methods for Handling Uncertainty}
\subsection{Bayesian Methods}
\subsubsection{Bayesian Methods Overview}\hfill\\
In the realm of UQ, Bayesian methods stand as a pivotal approach, offering a powerful framework for dealing with uncertainty in complex systems. 
At the heart of Bayesian UQ is the application of Bayes' Theorem, which mathematically expresses how our belief or knowledge about a certain parameter or model is updated with evidence. 
The theorem is elegantly encapsulated in the formula: 
\begin{equation}
    \label{eq:Bayes_Theorem}
    P(\boldsymbol{\theta}|D)=\frac{P(D|\boldsymbol{\theta})P(\boldsymbol{\theta})}{P(D)},
\end{equation}
%P(θ|D) = [P(D|θ)P(θ)] / P(D), 
where $P(\boldsymbol{\theta}|D)$ is the posterior probability of the parameter $\boldsymbol{\theta}$ given the data $D$, $P(D|\boldsymbol{\theta})$ is the likelihood of observing the data given the parameter, $P(\boldsymbol{\theta})$ is the prior probability of the parameter, and $P(D)$ is the marginal likelihood of the data. 
This Bayesian paradigm shifts the focus from a single 'true' model to a probabilistic distribution of models, reflecting all possible values that the parameters can take, weighted by their likelihood. 
By incorporating prior knowledge and continuously updating it with new data, Bayesian methods provide a dynamic and adaptive approach to UQ, allowing for a more comprehensive understanding of uncertainty in models.

Advancing from the foundational Bayesian principles, Bayesian Neural Networks (BNNs) offer an intricate method that integrates the flexibility of neural networks with the probabilistic rigor of Bayesian inference. 
BNNs are predicated on the notion of assigning probability distributions to the neural network's parameters $\boldsymbol{\theta}$, which encapsulates the model's uncertainty. 
This is a departure from traditional neural networks that often yield a single point estimate for parameters upon training completion.
During the training phase, BNNs aim to ascertain the posterior distribution of the parameters $P(\boldsymbol{\theta}|D)$. 
This is a result of updating the prior distribution $P(\boldsymbol{\theta})$ with the likelihood of the observed data $P(D|\boldsymbol{\theta})$, as Eq. (\ref{eq:Bayes_Theorem}).
In the context of predictions, BNNs leverage Bayesian model averaging during the test phase. 
This is not merely based on a singular set of parameters but rather an aggregation of multiple parameter sets sampled from the posterior distribution. 
For a given test sample $\mathbf{x}^*$, the prediction thereby incorporates the model uncertainty, yielding the predictive distribution:
\begin{equation}
    \label{eq:Bayes_prediction}
    p(y^* | \mathbf{x}^*, D) = \int p(y^* | \mathbf{x}^*, \boldsymbol{\theta}) p(\boldsymbol{\theta} | D) d\boldsymbol{\theta}.
\end{equation}
The quantification of uncertainty is elegantly captured and decomposed by the relationship \cite{depeweg2018decomposition}:
\begin{equation}
\label{eq:Bayes_AU_EU}
    \underbrace{I(y^*, \boldsymbol{\theta} | \mathbf{x}^*, D)}_{Epistemic} = \underbrace{\mathcal{H}\left[ \mathbb{E}_{P(\boldsymbol{\theta}|D)}\left[P(y^*|\mathbf{x}^*, \boldsymbol{\theta})\right]\right]}_{Entropy} - \underbrace{\mathbb{E}_{P(\boldsymbol{\theta}|D)}\left[\mathcal{H}\left[P(y^*|\mathbf{x}^*, \boldsymbol{\theta})\right]\right]}_{Aleatoric},
\end{equation}
where $\mathcal{H}(\cdot)$ is Shannon’s entropy of a probability distribution.
The \textit{entropy} term represents the total uncertainty in the prediction while the \textit{aleatoric} term reflects the inherent noise in the data that cannot be reduced with more information. 
The subtraction of aleatoric uncertainty from total uncertainty provides the measure of epistemic uncertainty, which denotes the uncertainty in the model's parameters and reduces as more data becomes available. 
This mathematical demarcation between the uncertainties empowers BNNs to provide a nuanced understanding of prediction reliability and decision-making under uncertainty.

\subsubsection{Bayesian Methods for Graphs}\hfill\\
Bayesian methods for semi-supervised node classification on graphs focus on obtaining the posterior probability distribution of nodes. One route lies in assigning a prior probability distribution to each node, then updating it with evidence propagated from other nodes to obtain the posterior distribution. Yamaguchi et al. \cite{yamaguchi2015socnl} assumed the label is a categorical random variable as $P(\hat{y}_i=k|\boldsymbol{\theta})$, where $\boldsymbol{\theta}$
 is the parameter of the categorical distribution. 
 Based on the smoothness hypothesis, they believed that a neighbor of node $i$ shares the same parameter $\boldsymbol{\theta}$ as $i$. 
 This leads to the multinomial likelihood function of labels of neighbors of $i$ $P(\hat{N}_i|\boldsymbol{\theta})\propto\Pi_{k=1}^K\theta_k^{n_{ik}}$ and the conjugate Dirichlet prior $P(\boldsymbol{\theta})\propto\Pi_{k=1}^K\theta_k^{\alpha_k-1}$
 , where $n_{ik}$ is the number of i’s neighbors whose label is $k$, and $\boldsymbol{\alpha}=(\alpha_1,\alpha_2,\cdots,\alpha_K)^T$ is the parameter of Dirichlet distribution. 
 Combining these, the posterior distribution of $\boldsymbol{\theta}$ is $P(\boldsymbol{\theta}|\hat{N}_i)\propto\Pi_{k=1}^K\theta_k^{\alpha_k+n_{ik}-1}$. Beyond the smoothness hypothesis and label propagation, Eswaran et al. \cite{eswaran2017power} used compatibility matrices to represent the strength of connections between nodes and propagates multinomial messages to support both homophily and heterophily network effects. 
 To handle high-dimensional features of nodes, Stadler et al. \cite{stadler2021graph} employed MLP encoding and computes node-level pseudo-counts, then propagates them via PPR-based message passing. All three methods utilize Dirichlet priors and posteriors, with the main difference being in the form of multinomial distribution evidence propagated.

Another route lies in transferring general Bayesian neural networks to GNNs. These works consider model parameters, passed messages, etc., as distributions rather than fixed values, and propose a series of Bayesian Graph Neural Networks (BGNNs).
To model the propagation of uncertainty in message-passing mechanisms, Xu et al. \cite{xu2022uncertainty} treated messages as multivariate Gaussian variables and employed GNNs to predict their means. To align with the intuition that more neighboring nodes provide more evidence, they defined an uncertainty propagation mechanism in GNNs for predicting the covariance of Gaussian distributions, which differs from traditional message passing. The covariance ultimately provides the uncertainty of predictions. 
Munikoti et al. \cite{munikoti2023general} considered probabilistic links, noise in node features, and modeling errors, treating node features, links between nodes, and model parameters as variables. They propagated aleatoric  uncertainty to the output by defining mechanisms for the propagation of means and variances of the node embeddings in GNN, while also estimating epistemic uncertainty through MC dropout. 
Zhao et al. \cite{zhao2020uncertainty} viewed model parameters as variables and, based on the methods of decomposing epistemic uncertainty and aleatoric uncertainty in Bayesian deep learning \cite{depeweg2018decomposition, malinin2018predictive}, uses MC dropout to obtain both types of uncertainty. Additionally, this work incorporates uncertainty from evidence theory, namely \textit{vacuity} due to lack of evidence and \textit{dissonance} due to conflicting evidence, offering a richer representation of uncertainty.
 %Another route lies in transferring general Bayesian neural networks to GNNs. These works consider model parameters, passed messages, etc., as distributions rather than fixed values, and propose a series of Bayesian Graph Neural Networks (BGNNs).
 %Munikoti et al. \cite{munikoti2023general} proposed a Bayesian graph neural network framework for quantifying aleatoric and epistemic uncertainties. They derived the relationship between the mean and variance of neurons in one layer of the GNN with the previous layer to propagate aleatoric uncertainty to the model output, and obtained epistemic uncertainty through Monte Carlo dropout.

%In the field of graph learning, Probabilistic Graphical Models (PGM) and Graph Neural Networks (GNNs) are two predominant methods for processing graph data. 
%This section focuses on exploring various Uncertainty Quantification (UQ) techniques that are specifically designed to measure and quantify the predictive uncertainties of these models. 
%In this realm, the targets of UQ can vary widely, including but not limited to node or graph classification predictions, and quantities of interest (QoIs) relevant to specific learning tasks. 
%\subsection{Uncertainty Quantification in Probabilistic Graphical Models}

%Applying PGMs to graph data involves probabilistic reasoning and learning on graph-structured data. 
%In this context, nodes and edges in the graph represent random variables and the probabilistic relationships between these variables, respectively. 
%This probabilistic modeling of the graph enables the use of PGM-specific inference and learning algorithms for various tasks like node classification, link prediction, and more. 
%Simultaneously, the UQ methods for PGMs can naturally provide uncertainty estimates for predictions related to these graph learning tasks. 

%\subsubsection{UQ of Marginal Probabilities for Variable Node} 

%\subsubsection{UQ of QoIs}
%\subsection{Uncertainty Quantification in Graph Neural Networks}
